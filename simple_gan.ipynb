{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_length = 1024\n",
    "train_data = torch.zeros((train_data_length, 2))\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[:, 0] = 2 * math.pi * torch.rand(train_data_length)\n",
    "train_data[:, 1] = torch.sin(train_data[:, 0])\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = torch.zeros(train_data_length)\n",
    "train_set = [\n",
    "    (train_data[i], train_labels[i]) for i in range(train_data_length)\n",
    "]\n",
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_data[:, 0], train_data[:, 1], \".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, you create a data loader called train_loader, \n",
    "# which will shuffle the data from train_set and return batches of 32 samples \n",
    "# that you’ll use to train the neural networks.\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=batch_size, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The discriminator is a model with a two-dimensional input and a one-dimensional output. \n",
    "# It’ll receive a sample from the real data or from the generator and \n",
    "# will provide the probability that the sample belongs to the real training data.\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, 256),    # linear (size of input, size of output)\n",
    "            nn.ReLU(),\n",
    "            # You use dropout to avoid overfitting.\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            # The output is composed of a single neuron with sigmoidal activation to represent a probability.\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator represents an instance of the neural network you’ve defined and is ready to be trained\n",
    "discriminator = Discriminator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The generator is the model that takes samples from a latent space \n",
    "# as its input and generates data resembling the data in the training set. \n",
    "# In this case, it’s a model with a two-dimensional input, \n",
    "# which will receive random points(z₁, z₂), and a two-dimensional output \n",
    "# that must provide(x̃₁, x̃₂) points resembling those from the training data.\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "generator = Generator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "num_epochs = 300\n",
    "loss_function = nn.BCELoss()\n",
    "# The binary cross-entropy function is a suitable loss function for training the discriminator \n",
    "# because it considers a binary classification task. It’s also suitable for training the generator \n",
    "# since it feeds its output to the discriminator, which provides a binary observable output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "optimizer_generator = torch.optim.Adam(generator.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for n, (real_samples, _) in enumerate(train_loader):\n",
    "        # Data for training the discriminator\n",
    "        real_samples_labels = torch.ones((batch_size, 1))\n",
    "        latent_space_samples = torch.randn((batch_size, 2))\n",
    "        generated_samples = generator(latent_space_samples)\n",
    "        generated_samples_labels = torch.zeros((batch_size, 1))\n",
    "        all_samples = torch.cat((real_samples, generated_samples))\n",
    "        all_samples_labels = torch.cat(\n",
    "            (real_samples_labels, generated_samples_labels)\n",
    "        )\n",
    "\n",
    "        # Training the discriminator\n",
    "        discriminator.zero_grad()\n",
    "        output_discriminator = discriminator(all_samples)\n",
    "        loss_discriminator = loss_function(\n",
    "            output_discriminator, all_samples_labels)\n",
    "        loss_discriminator.backward()\n",
    "        optimizer_discriminator.step()\n",
    "\n",
    "        # Data for training the generator\n",
    "        latent_space_samples = torch.randn((batch_size, 2))\n",
    "\n",
    "        # Training the generator\n",
    "        generator.zero_grad()\n",
    "        generated_samples = generator(latent_space_samples)\n",
    "        output_discriminator_generated = discriminator(generated_samples)\n",
    "        loss_generator = loss_function(\n",
    "            output_discriminator_generated, real_samples_labels\n",
    "        )\n",
    "        loss_generator.backward()\n",
    "        optimizer_generator.step()\n",
    "\n",
    "        # Show loss\n",
    "        if epoch % 10 == 0 and n == batch_size - 1:\n",
    "            print(f\"Epoch: {epoch} Loss D.: {loss_discriminator}\")\n",
    "            print(f\"Epoch: {epoch} Loss G.: {loss_generator}\")\n",
    "print(\"Training Complete !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Line 2: You get the real samples of the current batch from the data loader and assign them to real_samples. Notice that the first dimension of the tensor has the number of elements equal to batch_size. This is the standard way of organizing data in PyTorch, with each line of the tensor representing one sample from the batch.\n",
    "\n",
    "Line 4: You use torch.ones() to create labels with the value 1 for the real samples, and then you assign the labels to real_samples_labels.\n",
    "\n",
    "Lines 5 and 6: You create the generated samples by storing random data in latent_space_samples, which you then feed to the generator to obtain generated_samples.\n",
    "\n",
    "Line 7: You use torch.zeros() to assign the value 0 to the labels for the generated samples, and then you store the labels in generated_samples_labels.\n",
    "\n",
    "Lines 8 to 11: You concatenate the real and generated samples and labels and store them in all_samples and all_samples_labels, which you’ll use to train the discriminator.\n",
    "\n",
    "Next, in lines 14 to 19, you train the discriminator:\n",
    "\n",
    "Line 14: In PyTorch, it’s necessary to clear the gradients at each training step to avoid accumulating them. You do this using .zero_grad().\n",
    "\n",
    "Line 15: You calculate the output of the discriminator using the training data in all_samples.\n",
    "\n",
    "Lines 16 and 17: You calculate the loss function using the output from the model in output_discriminator and the labels in all_samples_labels.\n",
    "\n",
    "Line 18: You calculate the gradients to update the weights with loss_discriminator.backward().\n",
    "\n",
    "Line 19: You update the discriminator weights by calling optimizer_discriminator.step().\n",
    "\n",
    "Next, in line 22, you prepare the data to train the generator. You store random data in latent_space_samples, with a number of lines equal to batch_size. You use two columns since you’re providing two-dimensional data as input to the generator.\n",
    "\n",
    "You train the generator in lines 25 to 32:\n",
    "\n",
    "Line 25: You clear the gradients with .zero_grad().\n",
    "\n",
    "Line 26: You feed the generator with latent_space_samples and store its output in generated_samples.\n",
    "\n",
    "Line 27: You feed the generator’s output into the discriminator and store its output in output_discriminator_generated, which you’ll use as the output of the whole model.\n",
    "\n",
    "Lines 28 to 30: You calculate the loss function using the output of the classification system stored in output_discriminator_generated and the labels in real_samples_labels, which are all equal to 1.\n",
    "\n",
    "Lines 31 and 32: You calculate the gradients and update the generator weights. Remember that when you trained the generator, you kept the discriminator weights frozen since you created optimizer_generator with its first argument equal to generator.parameters().\n",
    "\n",
    "Finally, on lines 35 to 37, you display the values of the discriminator and generator loss functions at the end of each ten epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can get some random samples from the latent space and \n",
    "# feed them to the generator to obtain some generated samples.\n",
    "latent_space_samples = torch.randn(100, 2)\n",
    "generated_samples = generator(latent_space_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then you can plot the generated samples and check if they resemble the training data. \n",
    "# Before plotting the generated_samples data, you’ll need to use .detach() to return a tensor\n",
    "#  from the PyTorch computational graph, which you’ll then use to calculate the gradients\n",
    "generated_samples = generated_samples.detach()\n",
    "plt.plot(generated_samples[:, 0], generated_samples[:, 1], \".\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aac16c045827a40694571c32a5eab65b3bafe70755c551bce7e5d51e33df358c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
